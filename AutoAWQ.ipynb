{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56912f8-70d1-44b1-9ff2-9dea4d7df1d1",
   "metadata": {},
   "source": [
    "# 1 install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f3a96d-7cbb-48cd-922e-eb9163ea5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cature\n",
    "!pip install transformers torch pandas\n",
    "!pip install \"autoawq<0.2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9600017b-cb35-4629-94a2-fd365a671b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10859cc2-2b28-4202-812c-30613e37f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"zhaolizhang/DeepSeek-R1-Distill-Qwen-7B-140k\"\n",
    "quant_path = \"DeepSeek-R1-Distill-Qwen-7B-140k-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775805d5-38e5-4887-84a8-c6c9cc1afa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39876689-e8bf-4f25-bcdf-4c4d2bcfd3a7",
   "metadata": {},
   "source": [
    "# 2 load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b51ec9-5620-48ec-95d6-fd2378b8fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240098c2-c779-40ce-b9a0-aeb63f7edaef",
   "metadata": {},
   "source": [
    "# 3 load calibration data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603b390c-16bd-4d0e-a6c2-3af570c6ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"calibration.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f556ede0-703e-4bfd-83c7-6803956e9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate token counts for 'problem' and 'solution'\n",
    "df['problem_token_count'] = df['problem'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df['solution_token_count'] = df['solution'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Add a new column for the total token count\n",
    "df['total_token_count'] = df['problem_token_count'] + df['solution_token_count']\n",
    "\n",
    "# Print statistics for all token count columns\n",
    "print(\"Problem Token Count Statistics:\")\n",
    "print(df['problem_token_count'].describe())\n",
    "\n",
    "print(\"\\nSolution Token Count Statistics:\")\n",
    "print(df['solution_token_count'].describe())\n",
    "\n",
    "print(\"\\nTotal Token Count Statistics:\")\n",
    "print(df['total_token_count'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efceb5cb-1896-4baa-91de-233f1e340bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = df['answer'].astype(int)\n",
    "dfTrain = df[df['total_token_count'] < max_seq_length ].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8fe3e-5426-4c2a-8fa2-8db5875bfef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "\n",
    "for i, row in dfTrain.iterrows():\n",
    "    if i == 100:\n",
    "        break\n",
    "    # Combine the problem and solution into a single sequence\n",
    "    combined_text = row[\"problem\"] + \" \" + row[\"solution\"]\n",
    "    \n",
    "    # Tokenize the combined text\n",
    "    tokens = tokenizer.encode(combined_text)\n",
    "    \n",
    "    # Truncate the tokens if they exceed the maximum sequence length\n",
    "    if len(tokens) > max_seq_length:\n",
    "        tokens = tokens[:max_seq_length]\n",
    "    \n",
    "    # Decode the tokens back to text\n",
    "    truncated_text = tokenizer.decode(tokens)\n",
    "    \n",
    "    # Append the truncated text to the data list\n",
    "    data.append(truncated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c09434-5c22-4a7a-80e8-ba6d3b0f98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for _, row in dfTrain.iterrows():\n",
    "#     # Combine the problem and solution into a single sequence\n",
    "#     combined_text = row[\"problem\"] + \" \" + row[\"solution\"]\n",
    "    \n",
    "#     # Tokenize the combined text\n",
    "#     tokens = tokenizer.encode(combined_text)\n",
    "    \n",
    "#     # Truncate the tokens if they exceed the maximum sequence length\n",
    "#     if len(tokens) > max_seq_length:\n",
    "#         tokens = tokens[:max_seq_len]\n",
    "    \n",
    "#     # Decode the tokens back to text\n",
    "#     truncated_text = tokenizer.decode(tokens)\n",
    "    \n",
    "#     # Create a conversation-like structure\n",
    "#     conversation = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": row[\"problem\"]},  # User asks the problem\n",
    "#         {\"role\": \"assistant\", \"content\": truncated_text}  # Assistant provides the solution\n",
    "#     ]\n",
    "    \n",
    "#     data.append(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a54096-55c6-424f-bcbc-537048b00a4a",
   "metadata": {},
   "source": [
    "# 4  load model and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557468b4-177d-4f51-83b3-841ea9140125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_name, device_map=\"auto\", safetensors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8334bb2-0d94-4800-b4d9-fe79385a3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843abe6-c8a9-464c-b4de-83aa8a471df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantize(tokenizer, quant_config=quant_config, calib_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683af6fa-3185-4589-87b2-a5bc05a75431",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f8565-e221-4ddb-b7ec-ffd77f4d9848",
   "metadata": {},
   "source": [
    "# 5 put to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3c137-b068-4ed8-9e14-618a2668e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cature\n",
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bdc876-eebf-4fb8-ada4-09dfd546e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Push model and tokenizer\n",
    "login(token=\"\")\n",
    "model.push_to_hub(\"zhaolizhang/DeepSeek-R1-Distill-Qwen-7B-140k-AWQ\")\n",
    "tokenizer.push_to_hub(\"zhaolizhang/DeepSeek-R1-Distill-Qwen-7B-140k-AWQ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b19b6-73d2-473d-9bf0-9a0fcc7ff5d1",
   "metadata": {},
   "source": [
    "# 6 put to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eedca3-165e-4f33-b557-b0b1706c6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r x.zip DeepSeek-R1-Distill-Qwen-7B-140k-AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a1931-7b5f-4102-822e-2c5c307cdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. Install kaggle\n",
    "# ==============================\n",
    "!pip install kaggle --quiet\n",
    "\n",
    "# ==============================\n",
    "# 2. Upload your kaggle.json (API token)\n",
    "#    Get it from https://www.kaggle.com/<YourUserName>/account\n",
    "# ==============================\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please choose the kaggle.json file you downloaded from your Kaggle account:\")\n",
    "uploaded = files.upload()  # This prompts a file chooser dialog\n",
    "\n",
    "# ==============================\n",
    "# 3. Move kaggle.json into the correct location and set permissions\n",
    "# ==============================\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# ==============================\n",
    "# 4. Create folder \"/content/x\" to simulate your data/models.\n",
    "#    (If you already have a folder with your files, skip this step.)\n",
    "# ==============================\n",
    "!mkdir -p /content/x\n",
    "!echo \"Sample model data\" > /content/x/example.txt\n",
    "\n",
    "# ==============================\n",
    "# 5. Zip the folder /content/x -> x.zip\n",
    "# ==============================\n",
    "# !zip -r x.zip /content/x\n",
    "\n",
    "# ==============================\n",
    "# 6. Create the dataset-metadata.json file\n",
    "#    Adjust values to your liking:\n",
    "#    - kaggle_username\n",
    "#    - dataset_title\n",
    "#    - dataset_slug\n",
    "# ==============================\n",
    "import json\n",
    "\n",
    "kaggle_username = \"zzllusa\"  # e.g. \"johnsmith\"\n",
    "dataset_title   = \"DeepSeek-R1-Distill-Qwen-7B-140k-AWQ\"\n",
    "dataset_slug    = \"Qwenm7B\"\n",
    "\n",
    "metadata = {\n",
    "    \"title\": dataset_title,\n",
    "    \"id\": f\"{kaggle_username}/{dataset_slug}\",\n",
    "    \"licenses\": [\n",
    "        {\n",
    "            \"name\": \"CC0-1.0\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"dataset-metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# ==============================\n",
    "# 7. Create a staging folder and move:\n",
    "#    - x.zip\n",
    "#    - dataset-metadata.json\n",
    "# ==============================\n",
    "!mkdir AWQ\n",
    "!mv x.zip AWQ\n",
    "!cp dataset-metadata.json AWQ\n",
    "\n",
    "# ==============================\n",
    "# 8. Create the Kaggle dataset (PRIVATE)\n",
    "# ==============================\n",
    "!kaggle datasets create -p AWQ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
